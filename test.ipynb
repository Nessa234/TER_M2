{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "data= pd.read_csv(\"/Users/vanessaguerrier/Downloads/projet_TER/ner_dataset.csv\",encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47960"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_sentence= data[\"Sentence #\"].unique()\n",
    "len(nb_sentence)\n",
    "\n",
    "# id_begin_sentences = [data.index[data[\"Sentence #\"] == x][0] for x in nb_sentence if pd.notna(x)]\n",
    "# id_begin_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "X= data[\"Word\"]\n",
    "y= data[\"Tag\"]\n",
    "id2label = {i: label for i, label in enumerate(y)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-11 12:33:38.625766: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "labels = y.unique()\n",
    "num_labels = len(y.unique())\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<model> ChronoNet-XL </model> is a novel deep learning architecture designed for sequential temporal data analysis, employing a hierarchical attention mechanism that allows for improved temporal resolution without excessive computational overhead. The model contains approximately <param> 4.2 billion </param> parameters and was trained on a heterogeneous cluster consisting of <materiel> 32 NVIDIA A100 GPUs </materiel> over a period of <train> 38 days </train>. The training was conducted in <country> Germany </country> in <annee> 2023 </annee>, leveraging mixed-precision training to optimize memory usage and speed. <model> ChronoNet-XL </model> demonstrates a remarkable capacity to model complex temporal dependencies, particularly in multivariate time-series applications. Its design emphasizes both efficiency and scalability, making it adaptable to industrial and research environments where data throughput is high. The training pipeline incorporated gradient checkpointing and dynamic learning rate schedules to maintain convergence stability. The modelâ€™s performance was benchmarked across multiple datasets, demonstrating consistent improvements in prediction accuracy and robustness under noisy conditions. While certain hyperparameters were optimized through grid search, others, such as dropout ratios in intermediate layers, remain under investigation. Early experiments indicate that scaling <model> ChronoNet-XL </model> beyond its current size could yield further improvements, though the computational costs are projected to increase non-linearly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def decomposition_en_phrase(text):\n",
    "    text = re.sub(r'\\s*,\\s*', ' , ', text)\n",
    "    text = re.sub(r'\\s*<\\s*', ' <', text)\n",
    "    text = re.sub(r'\\s*>\\s*', '> ', text)\n",
    "    text= re.sub(r'\\s*([;:])\\s*', r' \\1 ', text)\n",
    "    phrases = re.split(r'(?<!\\d)[.!?]+(?!\\d)', text)\n",
    "    phrases = [p.strip()+\" .\" for p in phrases if p.strip()]\n",
    "    return phrases\n",
    "\n",
    "\n",
    "def decomposition_en_list_mot(text):  # sourcery skip: for-append-to-extend, inline-immediately-returned-variable, list-comprehension\n",
    "    phrases=decomposition_en_phrase(text)\n",
    "    list_mot=[]\n",
    "    for phrase in phrases:\n",
    "        list_mot.append(phrase.split())\n",
    "    return list_mot\n",
    "\n",
    "def extraire_nom_balise(tag):\n",
    "    return re.sub(r'[</>]', '', tag)\n",
    "\n",
    "\n",
    "def labeliser(list_phrase):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for phrase in list_phrase:\n",
    "        fe ,la = [],[]\n",
    "        i = 0\n",
    "        n = len(phrase)\n",
    "\n",
    "        while i < n:\n",
    "            token = phrase[i]\n",
    "            if token.startswith(\"<\") and not token.startswith(\"</\"):\n",
    "                nom = extraire_nom_balise(token)\n",
    "                j = i + 1\n",
    "                while j < n and not phrase[j].startswith(\"</\"):\n",
    "                    j += 1\n",
    "                if j == n:\n",
    "                    i += 1\n",
    "                    continue\n",
    "                taille = j - i - 1\n",
    "                for k in range(taille):\n",
    "                    mot = phrase[i + 1 + k]\n",
    "                    fe.append(mot)\n",
    "                    if k == 0:\n",
    "                        la.append(f\"B-{nom}\")\n",
    "                    else:\n",
    "                        la.append(f\"I-{nom}\")\n",
    "                i = j + 1\n",
    "            else:\n",
    "                fe.append(token)\n",
    "                la.append(\"O\")\n",
    "            i += 1\n",
    "        features.append(fe)\n",
    "        labels.append(la)\n",
    "    return features, labels\n",
    "\n",
    "li=decomposition_en_list_mot(text)\n",
    "fe,la=labeliser(li)\n",
    "# for i in range(len(la)):\n",
    "#     print(fe[i])\n",
    "#     print(la[i])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(inputs.word_ids())\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "\n",
    "token_embeddings = outputs.last_hidden_state\n",
    "\n",
    "print(token_embeddings.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
